{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# EN.520.637 Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 6: Double SARSA and Double Q-learning  </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "## Deadline\n",
    "11:59 pm Nov 26th, 2020 \n",
    "\n",
    "##  Content\n",
    "1. Cliff walking example\n",
    "2. Car pole example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gym and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleMDP():\n",
    "    \n",
    "    def __init__(self, mu=-0.1, std=1, max_actions=10):\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        self.max_actions = max_actions\n",
    "        \n",
    "        # Action numbers\n",
    "        self.right, self.left = 0, 1\n",
    "        # Define actions available for each state\n",
    "        self.state_actions = {\n",
    "            'A': [self.right, self.left],\n",
    "            'B': [i for i in range(max_actions)],\n",
    "            'C': [self.right], \n",
    "            'D': [self.left] }\n",
    "\n",
    "        self.state_transitions = {\n",
    "            'A': {self.right: 'C',\n",
    "                  self.left: 'B'},\n",
    "            'B': {a: 'D' for a in range(max_actions)},\n",
    "            'C': {self.right: 'Done'},\n",
    "            'D': {self.left: 'Done'}\n",
    "        }\n",
    "        \n",
    "        self.state = 'A'\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state = self.state_transitions[self.state][action]\n",
    "        # reward = 0 for all transitions except from B to D\n",
    "        reward = np.random.normal(self.mu, self.std) if self.state == 'D' else 0\n",
    "        done = True if self.state == 'D' or self.state == 'C' else False\n",
    "        return self.state, reward, done, None\n",
    "    \n",
    "    def available_actions(self, state=None):\n",
    "        if state is None:\n",
    "            return self.state_actions[self.state]\n",
    "        else:\n",
    "            return self.state_actions[state]\n",
    "    \n",
    "    def sample_actions(self):\n",
    "        return np.random.choice(self.available_actions())\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 'A'\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simpleMDP()\n",
    "max_tests = 10000\n",
    "n_eps = 300\n",
    "eps = 0.1\n",
    "lr = 0.1\n",
    "\n",
    "left_count_q = np.zeros(n_eps)\n",
    "q_estimate = np.zeros(n_eps)\n",
    "t = 0\n",
    "s_1 = None\n",
    "while t < max_tests:\n",
    "    Q = {state: np.zeros(env.max_actions) for state in env.state_actions.keys()}\n",
    "    for ep in range(n_eps):\n",
    "        s_0 = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            if np.random.uniform() < eps:\n",
    "                action = env.sample_actions()\n",
    "            else:\n",
    "                # Break ties among max values randomly if ties exist\n",
    "                # If no ties exist, the max will be selected with prob=1\n",
    "                max_qs = np.where(\n",
    "                    np.max(Q[s_0][env.available_actions()])==\n",
    "                        Q[s_0][env.available_actions()])[0]\n",
    "                action = np.random.choice(max_qs)\n",
    "                \n",
    "            # Count left actions from A\n",
    "            if s_0 == 'A' and action == 1:\n",
    "                left_count_q[ep] += 1\n",
    "\n",
    "            s_1, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update Q-Tables\n",
    "            Q[s_0][action] += lr * (reward + np.max(Q[s_1][env.available_actions()]) - \n",
    "                                    Q[s_0][action])\n",
    "            s_0 = s_1\n",
    "            if done:\n",
    "                q_estimate[ep] += (Q['A'][env.left] - q_estimate[ep]) / (ep + 1)\n",
    "                break\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Q-Learning\n",
    "env = simpleMDP()\n",
    "import copy\n",
    "left_count_dq = np.zeros(n_eps)\n",
    "q1_estimate = np.zeros(n_eps)\n",
    "q2_estimate = np.zeros(n_eps)\n",
    "t = 0\n",
    "s_1 = None\n",
    "while t < max_tests:\n",
    "    Q1 = {state: np.zeros(env.max_actions) for state in env.state_actions.keys()}\n",
    "    Q2 = copy.deepcopy(Q1)\n",
    "    for ep in range(n_eps):\n",
    "        s_0 = env.reset()\n",
    "        while True:\n",
    "            # Select eps-greedy action\n",
    "            if np.random.uniform() < eps or ep == 0:\n",
    "                action = env.sample_actions()\n",
    "            else:\n",
    "                # If no ties exist, the max will be selected with prob=1\n",
    "                Q_sum = Q1[s_0][env.available_actions()] + \\\n",
    "                        Q2[s_0][env.available_actions()]\n",
    "                max_qs = np.where(np.max(Q_sum)==Q_sum)[0]\n",
    "                action = np.random.choice(max_qs)\n",
    "                \n",
    "            # Count left actions from A\n",
    "            if s_0 == 'A' and action == 1:\n",
    "                left_count_dq[ep] += 1\n",
    "\n",
    "            s_1, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update Q-Tables\n",
    "            if np.random.uniform() < 0.5:\n",
    "                Q1[s_0][action] += lr * (reward + \\\n",
    "                    Q2[s_1][np.argmax(Q1[s_1][env.state_actions[s_1]])] \n",
    "                                         - Q1[s_0][action])\n",
    "            else:\n",
    "                Q2[s_0][action] += lr * (reward + \\\n",
    "                    Q1[s_1][np.argmax(Q2[s_1][env.state_actions[s_1]])] \\\n",
    "                                         - Q2[s_0][action])\n",
    "            s_0 = s_1\n",
    "            if done:\n",
    "                q1_estimate[ep] += (Q1['A'][env.left] - q1_estimate[ep]) / (ep + 1)\n",
    "                q2_estimate[ep] += (Q2['A'][env.left] - q2_estimate[ep]) / (ep + 1)\n",
    "                break\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.arange(env.max_actions)\n",
    "eps = 0.1\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = np.arange(len(left_count_q)), y= left_count_q/max_tests * 100,\n",
    "                         mode = 'lines', line = dict(width = 2, color = 'red'), name = 'Q-Learning'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x = np.arange(len(left_count_dq)), y= left_count_dq/max_tests * 100,\n",
    "                         mode = 'lines', line = dict(width = 2, color = 'blue'), name = 'Double Q-Learning'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x = np.arange(len(left_count_q)), y= np.ones(len(left_count_q))*eps/len(actions) * 100,\n",
    "                         mode = 'lines', line = dict(width = 2, color = 'gray'), name = 'Optimal'))\n",
    "\n",
    "\n",
    "fig.add_annotation(x=200, y=eps/len(actions),\n",
    "            text=\"$\\epsilon/M$\" + \"M = 10\",\n",
    "            showarrow=True,\n",
    "            ax = 0, ay = -150,\n",
    "            arrowsize = 2,\n",
    "            arrowhead=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title='% of Left Actions',\n",
    "    title='Estimation Bias ($\\epsilon = 0.1$); M = 10',\n",
    "    hovermode=\"x\",\n",
    "    paper_bgcolor = 'rgba(0,0,0,0)',\n",
    "    plot_bgcolor = 'rgba(0,0,0,0)',\n",
    "    font = dict(size = 16, color = 'black'),\n",
    "    width = 800,\n",
    "    height = 500\n",
    ")\n",
    "fig.update_xaxes(title = 'Episode',showgrid=True, gridwidth=1.5, gridcolor='#DFDFDF', showline=True, linecolor = '#AFAFAF', linewidth = 2.5, nticks = 7)\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1.5, gridcolor='#DFDFDF', showline=True, linecolor = '#AFAFAF', linewidth = 2.5, nticks = 7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.arange(env.max_actions)\n",
    "eps = 0.1\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = np.arange(len(q_estimate)), y= q_estimate,\n",
    "                         mode = 'lines', line = dict(width = 2, color = 'red'), name = 'Q-Learning'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x = np.arange(len(q1_estimate)), y= 0.5*(q1_estimate + q2_estimate),\n",
    "                         mode = 'lines', line = dict(width = 2, color = 'blue'), name = 'Double Q-Learning'))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title='Estimated value',\n",
    "    title='Estimated Value of Choosing Left from State A',\n",
    "    hovermode=\"x\",\n",
    "    paper_bgcolor = 'rgba(0,0,0,0)',\n",
    "    plot_bgcolor = 'rgba(0,0,0,0)',\n",
    "    font = dict(size = 16, color = 'black'),\n",
    "    width = 800,\n",
    "    height = 500\n",
    ")\n",
    "fig.update_xaxes(title = 'Episode',showgrid=True, gridwidth=1.5, gridcolor='#DFDFDF', showline=True, linecolor = '#AFAFAF', linewidth = 2.5, nticks = 7)\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1.5, gridcolor='#DFDFDF', showline=True, linecolor = '#AFAFAF', linewidth = 2.5, nticks = 7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
